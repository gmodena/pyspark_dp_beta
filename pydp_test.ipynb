{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd61586b",
   "metadata": {},
   "source": [
    "### custom virtual env setup\n",
    "\n",
    "This notebook sidesteps jupyterhub, and manually instantiates a Spark Session with a custom virtualenv distributed to workers. The virtual env has been created as follows:\n",
    "\n",
    "```\n",
    "$ cd $HOME/pyspark_dp_beta\n",
    "$ pythom -m venv venv\n",
    "$ source venv/bin/activate\n",
    "$ pip install findspark jupyter python-dp numpy\n",
    "$ tar cvfz venv.tar.gz venv\n",
    "```\n",
    "Start a jupyter server from the virtual enviroment with\n",
    "\n",
    "```\n",
    "$ jupyter --no-browser\n",
    "```\n",
    "\n",
    "Warning: this approach is meant as a test, and deviates from Newpyter guidelines https://wikitech.wikimedia.org/wiki/Analytics/Systems/Jupyter-SWAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef026554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Assumes $HOME/pyspark_dp_beta/venv.tar.gz exists\n",
    "venv = os.path.join(os.environ['HOME'], 'pyspark_dp_beta/venv.tar.gz#venv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b597584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_HOME = os.environ.get(\"SPARK_HOME\", \"/usr/lib/spark2\")\n",
    "findspark.init(SPARK_HOME)\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = 'pyspark-medium — differential privacy pydp — htriedman'\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = './venv/venv/bin/python'\n",
    "spark = (\n",
    "        SparkSession.builder\n",
    "        .master('yarn-client')\n",
    "        .config('spark.driver.maxResultSize', '2048M')\n",
    "        .config('spark.dynamicAllocation.maxExecutors', '64')\n",
    "        .config('spark.executor.memory', '8g')\n",
    "        .config('spark.executor.cores', 4)\n",
    "        .config('spark.sql.shuffle.partitions', 256)\n",
    "        .config('spark.yarn.dist.archives', venv)\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get (page title, page id, project, country, actor signature) for Aug 15 2021 UTC6:00\n",
    "\n",
    "rdd = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  pageview_info['page_title'] as page_title,\n",
    "  page_id,\n",
    "  pageview_info['project'] as project,\n",
    "  geocoded_data['country'] as country,\n",
    "  actor_signature\n",
    "FROM wmf.pageview_actor\n",
    "WHERE year = 2021 AND month = 8 AND day = 15 AND hour = 6 AND page_id IS NOT NULL\n",
    "\"\"\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add laplace noise to a single number\n",
    "def add_laplace_noise(x, eps, sensitivity):\n",
    "    import pydp\n",
    "    return x + pydp.distributions.LaplaceDistribution(eps, sensitivity).sample()\n",
    "\n",
    "# add laplace noise to a spark rdd\n",
    "def add_laplace_noise_to_rdd(rdd, eps, max_partitions, max_per_partition):\n",
    "    eps_per_partition = eps / max_partitions\n",
    "    sensitivity_per_partition = max_per_partition\n",
    "    return rdd.map(lambda x: (x[0], add_laplace_noise(x[1], eps_per_partition, sensitivity_per_partition)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add gaussian noise to a single number\n",
    "def add_gaussian_noise(x, eps, delta, sensitivity):\n",
    "    import pydp\n",
    "    sigma_squared = (2 * math.log(1.25 / delta) * sensitivity**2) / (eps**2)\n",
    "    return x + pydp.distributions.GaussianDistribution(sigma_squared).sample()\n",
    "\n",
    "# add laplace noise to a spark rdd\n",
    "def add_gaussian_noise_to_rdd(rdd, eps, delta, max_partitions, max_per_partition):\n",
    "    eps_per_partition = eps / max_partitions\n",
    "    sensitivity_per_partition = max_per_partition\n",
    "    return rdd.map(lambda x: (x[0], add_gaussian_noise(x[1], eps_per_partition, delta, sensitivity_per_partition)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_threshold(eps, delta, max_partitions, max_per_partition):\n",
    "    eps_per_partition = eps / max_partitions\n",
    "    sensitivity_per_partition = max_per_partition\n",
    "    b = sensitivity_per_partition / eps_per_partition\n",
    "    return -b * math.log(2 * b * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a539cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do bounded DP count\n",
    "def do_count(rdd, eps, delta, max_partitions, max_per_partition, noise_kind):\n",
    "    # rekey to a tuple of (actor signature, page id)\n",
    "    # ((actor_signature, page_id), pageview)\n",
    "    dp_count_rdd = rdd.map(lambda x: ((x.actor_signature, x.page_id), [x]))\n",
    "\n",
    "    # randomly get a set of at most `max_per_partition` pageviews for each (actor signature, page id) tuple\n",
    "    # ((actor_signature, page_id), [pageview]) {max length of max_per_partition}\n",
    "    dp_count_rdd = dp_count_rdd.reduceByKey(lambda x, y: random.sample(x + y, min(len(x) + len(y), max_per_partition)))\n",
    "\n",
    "    # rekey to just actor signature\n",
    "    # (actor_signature, [pageview]) {with redundancies}\n",
    "    dp_count_rdd = dp_count_rdd.map(lambda x: ((x[0][0], x[1])))\n",
    "\n",
    "    # randomly get a set of at most `max_partitions` sets of pageviews for each actor signature\n",
    "    # (actor_signature, [pageview]) {max length of max_per_partition * max_partitions}\n",
    "    dp_count_rdd = dp_count_rdd.reduceByKey(lambda x, y: random.sample(x + y, min(len(x) + len(y), max_partitions)))\n",
    "\n",
    "    # drop actor signature as key\n",
    "    # ([pageview])\n",
    "    dp_count_rdd = dp_count_rdd.map(lambda x: x[1])\n",
    "\n",
    "    # unnest lists of pageviews using a flatmap\n",
    "    # (pageview)\n",
    "    dp_count_rdd = dp_count_rdd.flatMap(lambda x: x)\n",
    "\n",
    "    # now that contributions are bounded, count views per tuple\n",
    "    dp_count_rdd = dp_count_rdd.map(lambda x: ((x.project, x.country, x.page_id, x.page_title), 1))\n",
    "    dp_count_rdd = dp_count_rdd.reduceByKey(lambda x, y: (x + y))\n",
    "\n",
    "    if noise_kind == \"laplace\":\n",
    "        # add laplace noise to counts\n",
    "        dp_count_rdd = add_laplace_noise_to_rdd(dp_count_rdd, eps, max_partitions, max_per_partition)\n",
    "    elif noise_kind == \"gaussian\":\n",
    "        dp_count_rdd = add_gaussian_noise_to_rdd(dp_count_rdd, eps, delta, max_partitions, max_per_partition)\n",
    "\n",
    "    # filter tuples that have less than `min_number_of_views` views\n",
    "    dp_count_rdd = dp_count_rdd.filter(lambda x: x[1] >= calculate_threshold(delta, eps, max_partitions, max_per_partition))\n",
    "\n",
    "    # round view count to integers for readability\n",
    "    dp_count_rdd = dp_count_rdd.map(lambda x: (x[0], round(x[1], 0)))\n",
    "\n",
    "    dp_count_rdd.takeOrdered(200, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb3f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total contributions (aka sensitivity) = max_per_partition * max_partitions\n",
    "max_partitions = 5    # say that users can visit at most 5 pages\n",
    "max_per_partition = 2 # and for each page they can contribute at most 2 pageviews\n",
    "\n",
    "eps = 1\n",
    "delta = 5e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_count(rdd, eps, delta, max_partitions, max_per_partition, \"laplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_count(rdd, eps, delta, max_partitions, max_per_partition, \"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b8e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defd2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^note that in the above output, `/srv/home/htriedman/PyDP/src` is included along with a bunch of\n",
    "# conda-related paths. The directory containing PyDP isn't getting sent to worker nodes with the rest\n",
    "# of the environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
